{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Table Detection with Fast RCNN\n",
    "\n",
    "By taking an ImageNet-pretrained model such as the VGG16, we can add a few more convolutional layers to construct an RPN, or region proposal network. This module extracts regions of interest, or RoIs, that inform a model on where to identify an object. \n",
    "\n",
    "\n",
    "When the RoIs are applied, we do max pooling only in the regions of interest, as to find an embedding that uniquely identifies that area of the input and well as building a description of what object might lie in that region. From this description, the model can then categorize that region into one of k categories it was trained to recognize. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Fast RCNN\n",
    "\n",
    "import logging\n",
    "import pprint\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from rcnn.config import config, default, generate_config\n",
    "from rcnn.symbol import *\n",
    "from rcnn.core import callback, metric\n",
    "from rcnn.core.loader import AnchorLoader\n",
    "from rcnn.core.module import MutableModule\n",
    "from rcnn.utils.load_data import load_gt_roidb, merge_roidb, filter_roidb\n",
    "from rcnn.utils.load_model import load_param\n",
    "\n",
    "\n",
    "def train_net(args, ctx, pretrained, epoch, prefix, begin_epoch, end_epoch,\n",
    "              lr=0.001, lr_step='5'):\n",
    "    # set up logger\n",
    "    logging.basicConfig()\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # setup config\n",
    "    config.TRAIN.BATCH_IMAGES = 1\n",
    "    config.TRAIN.BATCH_ROIS = 128\n",
    "    config.TRAIN.END2END = True\n",
    "    config.TRAIN.BBOX_NORMALIZATION_PRECOMPUTED = True\n",
    "\n",
    "    # load symbol\n",
    "    sym = eval('get_' + args.network + '_train')(num_classes=config.NUM_CLASSES, num_anchors=config.NUM_ANCHORS)\n",
    "    feat_sym = sym.get_internals()['rpn_cls_score_output']\n",
    "\n",
    "\n",
    "    # setup multi-gpu\n",
    "    batch_size = len(ctx)\n",
    "    input_batch_size = config.TRAIN.BATCH_IMAGES * batch_size\n",
    "\n",
    "    # print config\n",
    "    pprint.pprint(config)\n",
    "\n",
    "    # load dataset and prepare imdb for training\n",
    "    image_sets = [iset for iset in args.image_set.split('+')]\n",
    "    roidbs = [load_gt_roidb(args.dataset, image_set, args.root_path, args.dataset_path,\n",
    "                            flip=not args.no_flip)\n",
    "              for image_set in image_sets]\n",
    "    roidb = merge_roidb(roidbs)\n",
    "    roidb = filter_roidb(roidb)\n",
    "\n",
    "    # load training data\n",
    "    train_data = AnchorLoader(feat_sym, roidb, batch_size=input_batch_size, shuffle=not args.no_shuffle,\n",
    "                              ctx=ctx, work_load_list=args.work_load_list,\n",
    "                              feat_stride=config.RPN_FEAT_STRIDE, anchor_scales=config.ANCHOR_SCALES,\n",
    "                              anchor_ratios=config.ANCHOR_RATIOS, aspect_grouping=config.TRAIN.ASPECT_GROUPING)\n",
    "\n",
    "    # infer max shape\n",
    "    max_data_shape = [('data', (input_batch_size, 3, max([v[0] for v in config.SCALES]), max([v[1] for v in config.SCALES])))]\n",
    "    max_data_shape, max_label_shape = train_data.infer_shape(max_data_shape)\n",
    "    max_data_shape.append(('gt_boxes', (input_batch_size, 100, 5)))\n",
    "    print('providing maximum shape', max_data_shape, max_label_shape)\n",
    "\n",
    "    # infer shape\n",
    "    data_shape_dict = dict(train_data.provide_data + train_data.provide_label)\n",
    "    arg_shape, out_shape, aux_shape = sym.infer_shape(**data_shape_dict)\n",
    "    arg_shape_dict = dict(zip(sym.list_arguments(), arg_shape))\n",
    "    out_shape_dict = dict(zip(sym.list_outputs(), out_shape))\n",
    "    aux_shape_dict = dict(zip(sym.list_auxiliary_states(), aux_shape))\n",
    "    print('output shape')\n",
    "    pprint.pprint(out_shape_dict)\n",
    "\n",
    "    # load and initialize params\n",
    "    if args.resume:\n",
    "        arg_params, aux_params = load_param(prefix, begin_epoch, convert=True)\n",
    "    else:\n",
    "        arg_params, aux_params = load_param(pretrained, epoch, convert=True)\n",
    "        arg_params['rpn_conv_3x3_weight'] = mx.random.normal(0, 0.01, shape=arg_shape_dict['rpn_conv_3x3_weight'])\n",
    "        arg_params['rpn_conv_3x3_bias'] = mx.nd.zeros(shape=arg_shape_dict['rpn_conv_3x3_bias'])\n",
    "        arg_params['rpn_cls_score_weight'] = mx.random.normal(0, 0.01, shape=arg_shape_dict['rpn_cls_score_weight'])\n",
    "        arg_params['rpn_cls_score_bias'] = mx.nd.zeros(shape=arg_shape_dict['rpn_cls_score_bias'])\n",
    "        arg_params['rpn_bbox_pred_weight'] = mx.random.normal(0, 0.01, shape=arg_shape_dict['rpn_bbox_pred_weight'])\n",
    "        arg_params['rpn_bbox_pred_bias'] = mx.nd.zeros(shape=arg_shape_dict['rpn_bbox_pred_bias'])\n",
    "        arg_params['cls_score_weight'] = mx.random.normal(0, 0.01, shape=arg_shape_dict['cls_score_weight'])\n",
    "        arg_params['cls_score_bias'] = mx.nd.zeros(shape=arg_shape_dict['cls_score_bias'])\n",
    "        arg_params['bbox_pred_weight'] = mx.random.normal(0, 0.001, shape=arg_shape_dict['bbox_pred_weight'])\n",
    "        arg_params['bbox_pred_bias'] = mx.nd.zeros(shape=arg_shape_dict['bbox_pred_bias'])\n",
    "\n",
    "    # check parameter shapes\n",
    "    for k in sym.list_arguments():\n",
    "        if k in data_shape_dict:\n",
    "            continue\n",
    "        assert k in arg_params, k + ' not initialized'\n",
    "        assert arg_params[k].shape == arg_shape_dict[k], \\\n",
    "            'shape inconsistent for ' + k + ' inferred ' + str(arg_shape_dict[k]) + ' provided ' + str(arg_params[k].shape)\n",
    "    for k in sym.list_auxiliary_states():\n",
    "        assert k in aux_params, k + ' not initialized'\n",
    "        assert aux_params[k].shape == aux_shape_dict[k], \\\n",
    "            'shape inconsistent for ' + k + ' inferred ' + str(aux_shape_dict[k]) + ' provided ' + str(aux_params[k].shape)\n",
    "\n",
    "    # create solver\n",
    "    fixed_param_prefix = config.FIXED_PARAMS\n",
    "    data_names = [k[0] for k in train_data.provide_data]\n",
    "    label_names = [k[0] for k in train_data.provide_label]\n",
    "    mod = MutableModule(sym, data_names=data_names, label_names=label_names,\n",
    "                        logger=logger, context=ctx, work_load_list=args.work_load_list,\n",
    "                        max_data_shapes=max_data_shape, max_label_shapes=max_label_shape,\n",
    "                        fixed_param_prefix=fixed_param_prefix)\n",
    "\n",
    "    # decide training params\n",
    "    # metric\n",
    "    rpn_eval_metric = metric.RPNAccMetric()\n",
    "    rpn_cls_metric = metric.RPNLogLossMetric()\n",
    "    rpn_bbox_metric = metric.RPNL1LossMetric()\n",
    "    eval_metric = metric.RCNNAccMetric()\n",
    "    cls_metric = metric.RCNNLogLossMetric()\n",
    "    bbox_metric = metric.RCNNL1LossMetric()\n",
    "    eval_metrics = mx.metric.CompositeEvalMetric()\n",
    "    for child_metric in [rpn_eval_metric, rpn_cls_metric, rpn_bbox_metric, eval_metric, cls_metric, bbox_metric]:\n",
    "        eval_metrics.add(child_metric)\n",
    "        \n",
    "    # callback\n",
    "    batch_end_callback = callback.Speedometer(train_data.batch_size, frequent=args.frequent)\n",
    "    means = np.tile(np.array(config.TRAIN.BBOX_MEANS), config.NUM_CLASSES)\n",
    "    stds = np.tile(np.array(config.TRAIN.BBOX_STDS), config.NUM_CLASSES)\n",
    "    epoch_end_callback = callback.do_checkpoint(prefix, means, stds)\n",
    "    \n",
    "    # decide learning rate\n",
    "    base_lr = lr\n",
    "    lr_factor = 0.1\n",
    "    lr_epoch = [int(epoch) for epoch in lr_step.split(',')]\n",
    "    lr_epoch_diff = [epoch - begin_epoch for epoch in lr_epoch if epoch > begin_epoch]\n",
    "    lr = base_lr * (lr_factor ** (len(lr_epoch) - len(lr_epoch_diff)))\n",
    "    lr_iters = [int(epoch * len(roidb) / batch_size) for epoch in lr_epoch_diff]\n",
    "    print('lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters)\n",
    "    lr_scheduler = mx.lr_scheduler.MultiFactorScheduler(lr_iters, lr_factor)\n",
    "    # optimizer\n",
    "    optimizer_params = {'momentum': 0.9,\n",
    "                        'wd': 0.0005,\n",
    "                        'learning_rate': lr,\n",
    "                        'lr_scheduler': lr_scheduler,\n",
    "                        'rescale_grad': (1.0 / batch_size),\n",
    "                        'clip_gradient': 5}\n",
    "\n",
    "    # train\n",
    "    mod.fit(train_data, eval_metric=eval_metrics, epoch_end_callback=epoch_end_callback,\n",
    "            batch_end_callback=batch_end_callback, kvstore=args.kvstore,\n",
    "            optimizer='sgd', optimizer_params=optimizer_params,\n",
    "            arg_params=arg_params, aux_params=aux_params, \n",
    "            begin_epoch=begin_epoch, num_epoch=end_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Args\n",
    "class DictToObject:\n",
    "    '''\n",
    "    helper class to encapsulate all the args from dict to obj\n",
    "    '''\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = {'lr': 0.001, 'image_set': '2007_trainval', 'network': 'vgg',\n",
    "        'resume': False, 'pretrained': 'model/vgg16', 'root_path': 'data',\n",
    "        'dataset': 'PascalVOC', 'lr_step': '7', 'prefix': 'model/orge2e', \n",
    "        'end_epoch': 10, 'dataset_path': 'data/VOCdevkit', \n",
    "        'gpus': '0',\n",
    "        'no_flip': False, 'no_shuffle': False, 'begin_epoch': 0, \n",
    "        'work_load_list': None, 'pretrained_epoch': 0,\n",
    "        'kvstore': 'device', 'frequent': 20}\n",
    "\n",
    "args = DictToObject(**args)\n",
    "if len(args.gpus) > 1:\n",
    "    ctx = [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "else:\n",
    "    ctx = [mx.gpu(int(args.gpus))]\n",
    "train_net(args, ctx, args.pretrained, args.pretrained_epoch, args.prefix, args.begin_epoch, args.end_epoch,\n",
    "              lr=args.lr, lr_step=args.lr_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fast r-cnn trained on VOC2007 dataset\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from rcnn.config import config\n",
    "from rcnn.symbol import get_vgg_test, get_vgg_rpn_test\n",
    "from rcnn.io.image import resize, transform\n",
    "from rcnn.core.tester import Predictor, im_detect, im_proposal, vis_all_detection, draw_all_detection\n",
    "from rcnn.utils.load_model import load_param\n",
    "from rcnn.processing.nms import py_nms_wrapper, cpu_nms_wrapper, gpu_nms_wrapper\n",
    "\n",
    "import urllib2\n",
    "import tempfile\n",
    "\n",
    "# 20 classes\n",
    "CLASSES = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "config.TEST.HAS_RPN = True\n",
    "SHORT_SIDE = config.SCALES[0][0]\n",
    "LONG_SIDE = config.SCALES[0][1]\n",
    "PIXEL_MEANS = config.PIXEL_MEANS\n",
    "DATA_NAMES = ['data', 'im_info']\n",
    "LABEL_NAMES = None\n",
    "DATA_SHAPES = [('data', (1, 3, LONG_SIDE, SHORT_SIDE)), ('im_info', (1, 3))]\n",
    "LABEL_SHAPES = None\n",
    "\n",
    "# visualization\n",
    "CONF_THRESH = 0.7\n",
    "NMS_THRESH = 0.3\n",
    "nms = py_nms_wrapper(NMS_THRESH)\n",
    "\n",
    "\n",
    "def get_net(symbol, prefix, epoch, ctx):\n",
    "    arg_params, aux_params = load_param(prefix, epoch, convert=True, ctx=ctx, process=True)\n",
    "\n",
    "    # infer shape\n",
    "    data_shape_dict = dict(DATA_SHAPES)\n",
    "    arg_names, aux_names = symbol.list_arguments(), symbol.list_auxiliary_states()\n",
    "    arg_shape, _, aux_shape = symbol.infer_shape(**data_shape_dict)\n",
    "    arg_shape_dict = dict(zip(arg_names, arg_shape))\n",
    "    aux_shape_dict = dict(zip(aux_names, aux_shape))\n",
    "\n",
    "    # check shapes\n",
    "    for k in symbol.list_arguments():\n",
    "        if k in data_shape_dict or 'label' in k:\n",
    "            continue\n",
    "        assert k in arg_params, k + ' not initialized'\n",
    "        assert arg_params[k].shape == arg_shape_dict[k], \\\n",
    "            'shape inconsistent for ' + k + ' inferred ' + str(arg_shape_dict[k]) + ' provided ' + str(arg_params[k].shape)\n",
    "    for k in symbol.list_auxiliary_states():\n",
    "        assert k in aux_params, k + ' not initialized'\n",
    "        assert aux_params[k].shape == aux_shape_dict[k], \\\n",
    "            'shape inconsistent for ' + k + ' inferred ' + str(aux_shape_dict[k]) + ' provided ' + str(aux_params[k].shape)\n",
    "\n",
    "    predictor = Predictor(symbol, DATA_NAMES, LABEL_NAMES, context=ctx,\n",
    "                          provide_data=DATA_SHAPES, provide_label=LABEL_SHAPES,\n",
    "                          arg_params=arg_params, aux_params=aux_params)\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def generate_batch(im):\n",
    "    \"\"\"\n",
    "    preprocess image, return batch\n",
    "    :param im: cv2.imread returns [height, width, channel] in BGR\n",
    "    :return:\n",
    "    data_batch: MXNet input batch\n",
    "    data_names: names in data_batch\n",
    "    im_scale: float number\n",
    "    \"\"\"\n",
    "    im_array, im_scale = resize(im, SHORT_SIDE, LONG_SIDE)\n",
    "    im_array = transform(im_array, PIXEL_MEANS)\n",
    "    im_info = np.array([[im_array.shape[2], im_array.shape[3], im_scale]], dtype=np.float32)\n",
    "    data = [mx.nd.array(im_array), mx.nd.array(im_info)]\n",
    "    data_shapes = [('data', im_array.shape), ('im_info', im_info.shape)]\n",
    "    data_batch = mx.io.DataBatch(data=data, label=None, provide_data=data_shapes, provide_label=None)\n",
    "    return data_batch, DATA_NAMES, im_scale\n",
    "\n",
    "\n",
    "def demo_net(predictor, im, vis=False):\n",
    "    \"\"\"\n",
    "    generate data_batch -> im_detect -> post process\n",
    "    :param predictor: Predictor\n",
    "    :param image_name: image name\n",
    "    :param vis: will save as a new image if not visualized\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    data_batch, data_names, im_scale = generate_batch(im)\n",
    "    scores, boxes, data_dict = im_detect(predictor, data_batch, data_names, im_scale)\n",
    "\n",
    "    all_boxes = [[] for _ in CLASSES]\n",
    "    for cls in CLASSES:\n",
    "        cls_ind = CLASSES.index(cls)\n",
    "        cls_boxes = boxes[:, 4 * cls_ind:4 * (cls_ind + 1)]\n",
    "        cls_scores = scores[:, cls_ind, np.newaxis]\n",
    "        keep = np.where(cls_scores >= CONF_THRESH)[0]\n",
    "        dets = np.hstack((cls_boxes, cls_scores)).astype(np.float32)[keep, :]\n",
    "        keep = nms(dets)\n",
    "        all_boxes[cls_ind] = dets[keep, :]\n",
    "\n",
    "    boxes_this_image = [[]] + [all_boxes[j] for j in range(1, len(CLASSES))]\n",
    "\n",
    "    # print results\n",
    "    print('class ---- [[x1, x2, y1, y2, confidence]]')\n",
    "    for ind, boxes in enumerate(boxes_this_image):\n",
    "        if len(boxes) > 0:\n",
    "            print('---------', CLASSES[ind], '---------')\n",
    "            print(boxes)\n",
    "\n",
    "    if vis:\n",
    "        vis_all_detection(data_dict['data'].asnumpy(), boxes_this_image, CLASSES, im_scale)\n",
    "    else:\n",
    "        #result_file = image_name.replace('.', '_result.')\n",
    "        result_file = \"output.jpg\"\n",
    "        print('results saved to %s' % result_file)\n",
    "        im = draw_all_detection(data_dict['data'].asnumpy(), boxes_this_image, CLASSES, im_scale)\n",
    "        cv2.imwrite(result_file, im)\n",
    "\n",
    "def get_image_from_url(url, img_file):\n",
    "\n",
    "\n",
    "    req = urllib2.urlopen(url)\n",
    "    img_file.write(req.read())\n",
    "    img_file.flush()\n",
    "    return img_file.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - Lets run some predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = True\n",
    "gpu = 0\n",
    "epoch = 1\n",
    "prefix = 'orge2e'\n",
    "\n",
    "ctx = mx.gpu(gpu)\n",
    "symbol = get_vgg_test(num_classes=config.NUM_CLASSES, num_anchors=config.NUM_ANCHORS)\n",
    "predictor = get_net(symbol, prefix, epoch, ctx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = tempfile.NamedTemporaryFile()\n",
    "#url = 'http://images.all-free-download.com/images/graphiclarge/aeroplane_boeing_737_air_new_zealand_218019.jpg'\n",
    "#url = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21.jpg'\n",
    "#url = 'https://www.siemens.com/press/pool/de/pressebilder/2011/mobility/soimo201107/072dpi/soimo201107-04_072dpi.jpg'\n",
    "url = '/home/ubuntu/workspace/mxnet/example/rcnn/data/EXAMPLES/bike.jpg'\n",
    "\n",
    "if 'EXAMPLES' in url:\n",
    "    image = url\n",
    "else:\n",
    "    image = get_image_from_url(url, img_file)\n",
    "assert os.path.exists(image), image + ' not found'\n",
    "\n",
    "im = cv2.imread(image)\n",
    "demo_net(predictor, im, vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live object detection through webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from StringIO import StringIO\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "vis = False\n",
    "def classify(img):\n",
    "    img = img[len('data:image/png;base64,'):].decode('base64')\n",
    "    #img_stream = StringIO()\n",
    "    #img_stream.write(img)\n",
    "    #img_stream.seek(0)\n",
    "    #img_array = np.asarray(bytearray(img_stream.read()), dtype=np.uint8)\n",
    "    img_file = tempfile.NamedTemporaryFile()\n",
    "    img_file.write(img)\n",
    "    img_file.flush()\n",
    "    img_array = cv2.imread(img_file.name)\n",
    "    demo_net(predictor, img_array, vis)\n",
    "    return \"done\"\n",
    "    \n",
    "HTML(data=''' \n",
    "<style>\n",
    "#container {\n",
    "    margin: 0px auto;\n",
    "    width: 250px;\n",
    "    height: 250px;\n",
    "    border: 10px #333 solid;\n",
    "}\n",
    "#videoElement {\n",
    "    width: 250px;\n",
    "    height: 250px;\n",
    "    background-color: #666;\n",
    "}\n",
    "</style>\n",
    "<input type=button value=\"Capture Image\" onClick=\"take_snapshot()\">\n",
    "\n",
    "<video autoplay=\"true\" id=\"videoElement\"></video>\n",
    "<canvas id=\"c\" style=\"display:none;\" width=\"300\" height=\"300\"></canvas>\n",
    "<canvas id=\"display\" width=\"300\" height=\"300\"></canvas>\n",
    "\n",
    "\n",
    "<script>\n",
    "\n",
    "var video = document.querySelector(\"#videoElement\");\n",
    "navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia || navigator.oGetUserMedia;\n",
    " \n",
    "if (navigator.getUserMedia) {       \n",
    "    navigator.getUserMedia({video: true}, handleVideo, videoError);\n",
    "}\n",
    " \n",
    "function handleVideo(stream) {\n",
    "    video.src = window.URL.createObjectURL(stream);\n",
    "}\n",
    " \n",
    "function videoError(e) {\n",
    "    // do something\n",
    "}\n",
    "\n",
    "\n",
    "function handle_output(out) {\n",
    "    console.log(\"done classifying\", out);\n",
    "    // display image\n",
    "    var ctx = $(\"#display\")[0].getContext(\"2d\"),\n",
    "    img = new Image();\n",
    "    \n",
    "    img.onload = function(){\n",
    "        ctx.drawImage(img, 0, 0, 300, 300);\n",
    "        $(\"span\").text(\"Loaded.\");\n",
    "    };\n",
    "    img.src = \"output.jpg\";\n",
    "    \n",
    "    //console.log(\"done classifying\", out.content.data[\"text/plain\"]);\n",
    "}\n",
    "\n",
    "function take_snapshot() {\n",
    "    var canvas = document.getElementById('c');\n",
    "    var kernel = IPython.notebook.kernel;\n",
    "    canvas.getContext(\"2d\").drawImage(video, 0, 0, 300, 300);\n",
    "\n",
    "    data = canvas.toDataURL('image/png');\n",
    "    console.log(\"captured data\");\n",
    "    kernel.execute(\"classify('\" + data + \"')\", {\n",
    "        'iopub': {\n",
    "            'output': handle_output\n",
    "        }\n",
    "    }, {\n",
    "        silent: false\n",
    "    });\n",
    "}\n",
    "</script>\n",
    "\n",
    "''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
